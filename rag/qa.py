from dotenv import load_dotenv

from langchain.chains import RetrievalQA


class QA:
    """
    A class to perform question-answering using a retriever, language model (LLM), and a prompt.

    Attributes:
        compression_retriever: A retriever object used to fetch relevant documents for the query.
        llm: The language model (LLM) used to generate the response.
        prompt: The prompt template used by the LLM for generating responses.
        parser: A parser used to process the final answer after it's generated by the LLM.

    Methods:
        __init__(compression_retriever, llm, parser, prompt):
            Initializes the QA instance with the provided retriever, LLM, parser, and prompt.
        
        run_qa(query):
            Executes the question-answering process by retrieving relevant documents, generating a response, 
            and parsing the final answer.
    """
    
    def __init__(self, compression_retriever, llm, parser, prompt):
        """
        Initializes the QA instance with the given retriever, LLM, parser, and prompt.

        Args:
            compression_retriever: The retriever object to fetch relevant documents for the query.
            llm: The language model (LLM) that generates the response based on the retrieved documents.
            parser: The parser that processes the generated answer.
            prompt: The prompt used to guide the LLM in generating answers.

        """
        self.compression_retriever = compression_retriever
        self.llm = llm
        self.prompt = prompt
        self.parser = parser

    def run_qa(self, query):
        """
        Runs the question-answering process.

        This method:
        1. Uses the `compression_retriever` to fetch relevant documents based on the query.
        2. Uses the provided LLM to generate a response with the retrieved documents.
        3. Parses the generated answer using the `parser`.

        Args:
            query (str): The question to ask the model.

        Returns:
            result: The parsed result after generating the response with the LLM and parsing it with the provided parser.

        """
        qa = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.compression_retriever,
            chain_type_kwargs={
                "prompt": self.prompt,
                "document_variable_name": "context",
            },
        )
        answer = qa.run(query)
        result = self.parser.invoke(answer)
        return result
