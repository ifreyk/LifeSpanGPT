{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eriktadevosan/anaconda3/envs/dl/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in LLMConfig has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/eriktadevosan/LifespanGPT/rag/pipeline.py:7: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from rag.prompt_generator import PromptGeneratorConfig, PromptGenerator\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "from utils.parser import ParserConfig, Parser\n",
    "from rag.retriever import RetrieverConfig\n",
    "from rag.llm import LLMConfig\n",
    "from rag.pipeline import LifeSpanGPT\n",
    "\n",
    "load_dotenv()\n",
    "LLAMA_CLOUD = os.getenv('LLAMA_CLOUD')\n",
    "COHERE_TOKEN = os.getenv(\"COHERE_TOKEN\")\n",
    "OPENAI_TOKEN = os.getenv(\"OPENAI_TOKEN\")\n",
    "ROOT_PATH_ARTICLES = 'data' #folder with articles\n",
    "ROOT_PATH_RESULTS = 'pipeline_results/test'#folder to save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each article in ROOT_PATH_ARTICLES we parsing it into processed_data folder\n",
    "for file in tqdm(os.listdir(ROOT_PATH_ARTICLES)):\n",
    "    file_path = os.path.join(ROOT_PATH_ARTICLES,file)\n",
    "    parser_config = ParserConfig(path_to_file=file_path,\n",
    "                                llama_cloud_token=LLAMA_CLOUD,\n",
    "                                instruction=None)\n",
    "    parser = Parser(parser_config)\n",
    "    parser.create_parser()\n",
    "    parser.parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "molecules25225339\n",
      "Creating retriever\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eriktadevosan/anaconda3/envs/dl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/eriktadevosan/LifespanGPT/rag/retriever.py:56: LangChainDeprecationWarning: The class `CohereRerank` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import CohereRerank``.\n",
      "  compressor = CohereRerank(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/eriktadevosan/Library/Application Support/sagemaker/config.yaml\n",
      "Creating llm\n",
      "Generating prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eriktadevosan/LifespanGPT/rag/qa.py:23: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = qa.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What treatment or intervention or manipulation are used for mouse C57Bl/6 control male?\n",
      "Generating prompt...\n",
      "What are Lifespan or survival curve/results for mouse C57Bl/6 control male?\n",
      "Generating prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/59 [00:40<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m llm_config \u001b[38;5;241m=\u001b[39m LLMConfig(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, api_key\u001b[38;5;241m=\u001b[39mOPENAI_TOKEN)\n\u001b[1;32m     15\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m LifeSpanGPT(config, llm_config)\n\u001b[0;32m---> 16\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mROOT_PATH_RESULTS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     18\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(answer, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/LifespanGPT/rag/pipeline.py:92\u001b[0m, in \u001b[0;36mLifeSpanGPT.run_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m sub_key,sub_value \u001b[38;5;129;01min\u001b[39;00m subject\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     91\u001b[0m                         output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroups\u001b[39m\u001b[38;5;124m\"\u001b[39m][i][sub_key] \u001b[38;5;241m=\u001b[39m sub_value\n\u001b[0;32m---> 92\u001b[0m             \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for each file in processed_data run pipeline\n",
    "for file in tqdm(os.listdir(\"processed_data\")):\n",
    "    os.makedirs(ROOT_PATH_RESULTS, exist_ok=True)\n",
    "    file_name = file.split(\".md\")[0]\n",
    "    print(file_name)\n",
    "    config = RetrieverConfig(\n",
    "        file_path=f\"processed_data/{file}\",\n",
    "        embeding_model=\"BAAI/bge-small-en\",\n",
    "        reranker_model=\"rerank-english-v3.0\",\n",
    "        chunk_size=15000,\n",
    "        chunk_overlap=2000,\n",
    "        COHERE_TOKEN=COHERE_TOKEN,\n",
    "    )\n",
    "    llm_config = LLMConfig(model_name=\"gpt-4o\", temperature=0.0, api_key=OPENAI_TOKEN)\n",
    "    pipeline = LifeSpanGPT(config, llm_config)\n",
    "    answer = pipeline.run_pipeline()\n",
    "    with open(f\"{ROOT_PATH_RESULTS}/{file_name}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(answer, f, indent=4)\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
